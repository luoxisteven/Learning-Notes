## Natural Language Processing

## Transformer

Transformer 的结构包括一个编码器（Encoder）和解码器（Decoder），并可以有三种变体：
1. **单独编码器**结构。
2. **单独解码器**结构（如 GPT）。
3. **编码器和解码器都有**的完整结构。

GPT 就采用了**单独解码器**的结构。  
Transformer 的核心是**Attention**机制，它和其他 NLP 模型（如 RNN、LSTM 等）不同。传统的这些 NLP 模型在处理长序列时，由于信息传递的限制，前面的信息容易丢失。而 **Attention** 模型能够同时关注序列的不同位置，从而更好地解决长序列依赖问题，使模型表现提升。

参考资料：[Transformer 详解](https://zhuanlan.zhihu.com/p/607423406)


## 微调（Fine-tuning）

**微调**是指在一个已经预训练好的模型上进行少量任务特定的数据训练，使其适应特定任务。微调通常可以大幅提高模型在特定任务上的表现。

参考讨论：[什么是微调？](https://www.zhihu.com/question/298203515/answer/3355179300)


## Attention机制

**QKV**（Query, Key, Value）是 Attention 机制的核心思想。通过将输入映射为 Q（查询）、K（键）和 V（值），计算出注意力权重，从而捕捉输入的不同部分之间的相关性。

## BERT

BERT 是一个预训练模型，返回的是根据上下文推测的某个词的语义。  
BERT 的网络结构类似于 Transformer 的**编码器（Encoder）**部分，而 GPT 类似于 Transformer 的**解码器（Decoder）**部分。

**结构差异**：  
- BERT 使用 **Multi-Head-Attention**，而 GPT 使用的是 **Masked Multi-Head-Attention**。

更多内容参考：[BERT 模型解析](https://zhuanlan.zhihu.com/p/607605399)


## T5

T5（Text-to-Text Transfer Transformer）模型的核心思想是将**所有 NLP 任务都转化为 Text-to-Text**（文本到文本）的生成任务。无论是翻译、问答还是文本分类，T5 都将输入转换为文本，输出同样是文本。

更多内容参考：[T5 模型详解](https://zhuanlan.zhihu.com/p/88438851)


## NLP 预训练的各种模式

### 1. 自回归语言模型（Auto-Regressive Language Model）
根据之前的词预测下一个词，即通过学习上下文生成文本。

**典型模型**：GPT 系列（OpenAI GPT, GPT-2, GPT-3）

**训练目标**：最大化在给定前面词的情况下，下一个词出现的概率。

**优点**：适合生成任务（如对话生成、文本生成）。

**缺点**：由于是单向的，不能有效捕捉双向上下文信息，尤其在需要理解整体句子的任务中会有一定局限性。

### 2. 自编码语言模型（Auto-Encoding Language Model）
通过学习捕捉输入文本的双向上下文信息。恢复被遮盖或替换的部分（即掩码语言模型，Masked Language Model）。

**典型模型**：BERT

**训练目标**：随机遮蔽输入文本中的一部分词，模型的任务是根据剩余的上下文预测这些被遮蔽的词。

**优点**：能够有效捕捉到全局信息，适合自然语言理解任务，如问答、文本分类。

**缺点**：不擅长文本生成，因为它无法生成完整的句子。

### 3. 序列到序列（Seq2Seq）模型
结合了自回归和自编码的优点，使用编码器-解码器架构。编码器将输入文本转化为一个隐向量，解码器则基于隐向量生成输出文本。

**典型模型**：T5（Text-to-Text Transfer Transformer）、BART（Bidirectional and Auto-Regressive Transformers）

**训练目标**：
- **T5**：将所有 NLP 任务（如翻译、摘要、问答等）转化为文本到文本的生成任务。
- **BART**：使用自编码器掩码部分输入，并通过自回归解码器进行生成。

**优点**：适用于多种任务，兼顾理解和生成能力。

**缺点**：模型复杂度较高，训练和推理时间相对更长。

### 4. 生成对抗网络（Generative Adversarial Networks, GAN）模型
GAN 模型主要用于文本生成，通常涉及生成器和判别器两个网络。生成器负责生成文本，而判别器负责判定生成的文本是否真实，双方相互竞争、相互优化。

**典型模型**：TextGAN, SeqGAN

**训练目标**：生成器生成接近真实语料的文本，判别器评估生成的文本与真实文本的相似性。

**优点**：适用于文本生成任务，能够生成更加多样化的文本。

**缺点**：训练不稳定，容易发生模式崩溃（mode collapse），难以生成长文本。

### 5. 双向自回归模型（Bidirectional Auto-Regressive Model）
结合了自回归和自编码的优势，通过双向的上下文信息来生成文本。既可以利用前后文进行理解，也可以用来生成文本。

**典型模型**：XLNet

**训练目标**：通过生成多个不同的词序列顺序（Permutation Language Modeling），捕捉双向上下文信息，避免了 BERT 中的掩码机制和 GPT 中的单向性问题。

**优点**：兼顾语言理解和生成任务，适用于更复杂的 NLP 任务。

**缺点**：训练复杂度较高，对计算资源要求较大。

### 6. 对比学习模型（Contrastive Learning Models）
对比学习是一种自监督学习方法，利用相似和不同的样本对比来进行预训练。通常不依赖显式标签，通过设计任务生成正样本和负样本对，来学习有意义的表示。

**典型模型**：SimCSE, InfoBERT

**训练目标**：通过引入噪声和对比机制，使模型学习到更鲁棒的表示。

**优点**：对缺乏大量标注数据的任务非常有效，适合表示学习任务。

**缺点**：依赖高质量的对比对生成方式，可能存在难以找到好的对比策略的问题。

### 7. 跨模态预训练模型（Multimodal Pretraining Models）
跨模态模型不仅在文本上进行预训练，还结合其他模态（如图像、视频、音频等）信息进行多模态任务的训练。应用包括图像生成描述、视频字幕生成等。

**典型模型**：CLIP, DALL·E

**训练目标**：通过图文对齐或跨模态对齐任务，学习到不同模态之间的关联。

**优点**：适用于多模态任务，能够结合文本和其他模态信息，增强模型的感知能力。

**缺点**：模型复杂度更高，依赖更多类型的数据。


## 排列语言建模 Permutation Language Modeling

- BERT 预训练的两个问题：
    1) 上下游任务不一致：BERT用掩码语言模型（Masked Language Model, MLM时，上游掩码训练时会是用<Mask>对词汇进行替换，但是下游模型实际应用时，并没有这些<Mask>。前者带有人为添加的噪声，而后者则使用干净数据。前者的本质是构造一个去噪模型，而后者旨在完成分类等其他NLP任务。
    2) 忽略了被遮掩词汇之间的关系：在同一个句子多个掩码词时，在每次训练中仅关注单个被遮掩的词，而忽略了遮掩词之间的潜在依赖关系。

- XLNet旨在解决以上两个问题:
    - 核心在于通过不同的词序列顺序来预测每个词。动态生成不同序列的排列。
	- XLNet不再使用掩码，但是通过打乱语序，学习上下文。
	- $X = [x1, x2, x3, x4]$ 生成 $[x2, x4, x1, x3], [x3, x1, x4, x2], [x1, x3, x2, x4]$

